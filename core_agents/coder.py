#!/usr/bin/env python3
"""
Elite Coder Agent - Using Core Agent Infrastructure
==================================================

Professional AI agent specialized in creating world-class LangGraph agents,
tools, and multi-agent systems. Built on Core Agent's powerful infrastructure
including MemoryManager, ToolManager, and AgentConfig.
"""

import os
import sys
import json
import tempfile
import subprocess
from typing import Dict, List, Any, Optional
from datetime import datetime

# Add workspace to path for imports
sys.path.insert(0, '/workspace')

# Core Agent Infrastructure
from core.config import AgentConfig
from core.managers import MemoryManager
from core.tools import create_python_coding_tools
from langchain_openai import AzureChatOpenAI
from langchain_core.tools import BaseTool
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from pydantic import BaseModel, Field


class CoderConfig:
    """Elite Coder Agent Configuration - Using Real Azure OpenAI"""
    
    # Azure OpenAI Configuration (exactly as specified by user)
    AZURE_OPENAI_ENDPOINT = "https://oai-202-fbeta-dev.openai.azure.com/"
    OPENAI_API_KEY = "BDfLqbP0vVCTuRkXtE4Zy9mK7neLrJlHXlISgqJxVNTg2ca71EI5JQQJ99BDACfhMk5XJ3w3AAABACOGgIx4"
    OPENAI_API_VERSION = "2023-12-01-preview"
    GPT4_MODEL_NAME = "gpt-4"
    GPT4_DEPLOYMENT_NAME = "gpt4"
    
    # Elite Coder Parameters
    TEMPERATURE = 0.1
    MAX_TOKENS = 4000


class LangGraphTemplateInput(BaseModel):
    """Input schema for LangGraph template generation"""
    template_type: str = Field(description="Type: simple, with_tools, multi_agent")
    agent_name: str = Field(description="Name for the agent")
    purpose: str = Field(description="What the agent should do")
    tools_needed: List[str] = Field(default=[], description="List of tools if needed")


class LangGraphTemplateTool(BaseTool):
    """Elite tool for generating complete LangGraph agent templates"""
    
    name: str = "langgraph_generator"
    description: str = """Generate production-ready LangGraph agents with Azure OpenAI.
    Types: simple (basic agent), with_tools (agent with custom tools), multi_agent (supervisor system)
    Always includes proper Azure OpenAI configuration and error handling."""
    args_schema: type[BaseModel] = LangGraphTemplateInput
    
    def _run(self, template_type: str, agent_name: str, purpose: str, tools_needed: List[str] = []) -> str:
        """Generate elite LangGraph template based on type"""
        
        if template_type == "simple":
            return self._generate_simple_agent(agent_name, purpose)
        elif template_type == "with_tools":
            return self._generate_agent_with_tools(agent_name, purpose, tools_needed)
        elif template_type == "multi_agent":
            return self._generate_multi_agent_system(agent_name, purpose, tools_needed)
        else:
            return f"❌ Unknown template type: {template_type}. Use: simple, with_tools, multi_agent"
    
    def _generate_simple_agent(self, agent_name: str, purpose: str) -> str:
        """Generate elite simple LangGraph agent"""
        return f'''#!/usr/bin/env python3
"""
{agent_name} - Elite LangGraph Agent
Generated by Elite Coder Agent using Core Agent Infrastructure
Purpose: {purpose}
"""

from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from typing import TypedDict, List
from langchain_openai import AzureChatOpenAI
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class {agent_name}State(TypedDict):
    """Elite state management for {agent_name}"""
    messages: List[BaseMessage]
    input: str
    output: str
    error: str
    metadata: dict

def create_azure_openai_model():
    """Create Azure OpenAI model with elite configuration"""
    return AzureChatOpenAI(
        azure_endpoint="{CoderConfig.AZURE_OPENAI_ENDPOINT}",
        api_key="{CoderConfig.OPENAI_API_KEY}",
        api_version="{CoderConfig.OPENAI_API_VERSION}",
        model="{CoderConfig.GPT4_MODEL_NAME}",
        deployment_name="{CoderConfig.GPT4_DEPLOYMENT_NAME}",
        temperature={CoderConfig.TEMPERATURE},
        max_tokens={CoderConfig.MAX_TOKENS}
    )

def process_node(state: {agent_name}State) -> {agent_name}State:
    """Elite processing node for {agent_name}"""
    logger.info(f"{{agent_name}} processing input: {{state['input'][:100]}}")
    
    try:
        llm = create_azure_openai_model()
        
        # Elite system prompt for {purpose}
        system_prompt = f\"\"\"You are {agent_name}, an elite AI agent specialized in: {purpose}
        
You are powered by Azure OpenAI GPT-4 and built with Core Agent infrastructure principles.
Provide exceptional, detailed, and accurate responses that showcase expertise in your domain.

Key capabilities:
- Deep understanding of {purpose}
- Production-ready solutions
- Clear explanations and examples
- Error-resilient operations\"\"\"
        
                 # Enhanced conversation with system context
         messages = [
             {{"role": "system", "content": system_prompt}},
             {{"role": "user", "content": state["input"]}}
         ]
        
        response = llm.invoke(messages)
        
        return {{
            **state,
            "output": response.content,
            "error": "",
            "metadata": {{
                "agent_name": "{agent_name}",
                "purpose": "{purpose}",
                "timestamp": datetime.now().isoformat(),
                "model": "gpt-4"
            }}
        }}
        
    except Exception as e:
        logger.error(f"{{agent_name}} processing error: {{str(e)}}")
        return {{
            **state,
            "output": "",
            "error": f"Processing error: {{str(e)}}",
            "metadata": {{"error_type": type(e).__name__}}
        }}

def create_{agent_name.lower()}_agent():
    """Create elite {agent_name} agent with proper configuration"""
    logger.info(f"Creating elite {{agent_name}} agent for: {purpose}")
    
    workflow = StateGraph({agent_name}State)
    
    # Add the elite processing node
    workflow.add_node("process", process_node)
    
    # Set entry point
    workflow.set_entry_point("process")
    
    # End after processing
    workflow.add_edge("process", END)
    
    return workflow.compile()

# Elite usage example with error handling
if __name__ == "__main__":
    import datetime
    
    print(f"🚀 Initializing Elite {{agent_name}} Agent")
    print(f"Purpose: {purpose}")
    print(f"Powered by: Azure OpenAI GPT-4")
    print("=" * 60)
    
    try:
        agent = create_{agent_name.lower()}_agent()
        
        # Test the agent
        test_input = f"Hello! I need help with: {purpose}. Please provide a comprehensive response."
        
        result = agent.invoke({{
            "input": test_input,
            "messages": [],
            "output": "",
            "error": "",
            "metadata": {{}}
        }})
        
        if result["error"]:
            print(f"❌ Error: {{result['error']}}")
        else:
            print(f"✅ Success!")
            print(f"Response: {{result['output'][:200]}}...")
            print(f"Metadata: {{result['metadata']}}")
            
    except Exception as e:
        print(f"❌ Agent initialization failed: {{str(e)}}")
'''
    
    def _generate_agent_with_tools(self, agent_name: str, purpose: str, tools_needed: List[str]) -> str:
        """Generate elite agent with custom tools"""
        tools_code = []
        for tool in tools_needed:
            clean_tool_name = tool.lower().replace(" ", "_").replace("-", "_")
            tools_code.append(f'''
@tool
def {clean_tool_name}_tool(query: str) -> str:
    """Elite tool for {tool} - powered by Core Agent infrastructure"""
    try:
        # TODO: Implement elite {tool} functionality
        # This is a template - replace with actual implementation
        logger.info(f"Executing {tool} tool with query: {{query[:50]}}")
        
        # Placeholder implementation
        result = f"Elite {tool} processing completed for: {{query}}"
        
        return result
    except Exception as e:
        logger.error(f"{tool} tool error: {{str(e)}}")
        return f"❌ {tool} tool error: {{str(e)}}"''')
        
        tools_imports = "\\n".join(tools_code)
        tools_list = [f"{tool.lower().replace(' ', '_').replace('-', '_')}_tool" for tool in tools_needed]
        
        return f'''#!/usr/bin/env python3
"""
{agent_name} - Elite LangGraph Agent with Tools
Generated by Elite Coder Agent using Core Agent Infrastructure
Purpose: {purpose}
Tools: {', '.join(tools_needed)}
"""

from langgraph.graph import StateGraph, END
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, ToolMessage
from typing import TypedDict, List, Literal
from langchain_openai import AzureChatOpenAI
import logging
import json
from datetime import datetime

# Configure elite logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Elite Tools Implementation
{tools_imports}

class {agent_name}State(TypedDict):
    """Elite state for {agent_name} with advanced tool integration"""
    messages: List[BaseMessage]
    input: str
    selected_tool: str
    tool_result: str
    output: str
    error: str
    metadata: dict

def create_azure_openai_model():
    """Create Azure OpenAI model with elite configuration"""
    return AzureChatOpenAI(
        azure_endpoint="{CoderConfig.AZURE_OPENAI_ENDPOINT}",
        api_key="{CoderConfig.OPENAI_API_KEY}",
        api_version="{CoderConfig.OPENAI_API_VERSION}",
        model="{CoderConfig.GPT4_MODEL_NAME}",
        deployment_name="{CoderConfig.GPT4_DEPLOYMENT_NAME}",
        temperature={CoderConfig.TEMPERATURE},
        max_tokens={CoderConfig.MAX_TOKENS}
    )

def tool_selection_node(state: {agent_name}State) -> {agent_name}State:
    """Elite tool selection using GPT-4 intelligence"""
    logger.info(f"{{agent_name}} selecting optimal tool for: {{state['input'][:50]}}")
    
    try:
        llm = create_azure_openai_model()
        
        available_tools = {str(tools_list)}
        
        selection_prompt = f\"\"\"You are {agent_name}, an elite AI agent for: {purpose}
        
You have access to these specialized tools: {{available_tools}}

User request: {{state["input"]}}

Analyze the request and determine the BEST tool to use. Consider:
1. Which tool most directly addresses the user's need
2. The complexity and scope of the request
3. The expected output quality

Respond with ONLY the exact tool name from the list, or 'none' if no tool is needed.
If multiple tools could work, choose the most specialized one.\"\"\"
        
        response = llm.invoke(selection_prompt)
        selected = response.content.strip().lower()
        
        # Validate selection
        valid_tools = [tool.lower() for tool in available_tools] + ['none']
        if selected in valid_tools:
            logger.info(f"Selected tool: {{selected}}")
            return {{**state, "selected_tool": selected}}
        else:
            logger.warning(f"Invalid tool selection: {{selected}}, defaulting to none")
            return {{**state, "selected_tool": "none"}}
            
    except Exception as e:
        logger.error(f"Tool selection error: {{str(e)}}")
        return {{**state, "selected_tool": "none", "error": f"Tool selection failed: {{str(e)}}"}}

def tool_execution_node(state: {agent_name}State) -> {agent_name}State:
    """Elite tool execution with comprehensive error handling"""
    selected_tool = state["selected_tool"]
    
    if selected_tool == "none":
        logger.info("No tool execution needed")
        return {{**state, "tool_result": "Direct processing - no tool required"}}
    
    logger.info(f"Executing elite tool: {{selected_tool}}")
    
    try:
        # Execute the selected tool with elite error handling
        tool_result = ""
        
        {" elif ".join([f'''if selected_tool == "{tool.lower().replace(' ', '_').replace('-', '_')}":
            tool_result = {tool.lower().replace(' ', '_').replace('-', '_')}_tool.invoke({{"query": state["input"]}})''' for tool in tools_needed])}
        else:
            tool_result = f"❌ Tool '{{selected_tool}}' not found in available tools"
        
        logger.info(f"Tool execution completed: {{str(tool_result)[:100]}}")
        return {{**state, "tool_result": str(tool_result)}}
        
    except Exception as e:
        error_msg = f"Tool execution error: {{str(e)}}"
        logger.error(error_msg)
        return {{**state, "tool_result": error_msg, "error": str(e)}}

def finalize_node(state: {agent_name}State) -> {agent_name}State:
    """Elite response finalization using GPT-4"""
    logger.info(f"{{agent_name}} finalizing response")
    
    try:
        llm = create_azure_openai_model()
        
        final_prompt = f\"\"\"You are {agent_name}, an elite AI agent specialized in: {purpose}

You have processed a user request using your advanced capabilities:

Original Request: {{state["input"]}}
Tool Used: {{state["selected_tool"]}}
Tool Result: {{state["tool_result"]}}

Now provide a comprehensive, professional response that:
1. Directly addresses the user's request
2. Incorporates the tool results seamlessly
3. Provides additional context and insights
4. Maintains the highest quality standards

Deliver an exceptional response worthy of an elite AI agent.\"\"\"
        
        response = llm.invoke(final_prompt)
        
        metadata = {{
            "agent_name": "{agent_name}",
            "purpose": "{purpose}",
            "tool_used": state["selected_tool"],
            "timestamp": datetime.now().isoformat(),
            "model": "gpt-4",
            "tools_available": {tools_list}
        }}
        
        logger.info("Response finalization completed successfully")
        return {{**state, "output": response.content, "metadata": metadata}}
        
    except Exception as e:
        error_msg = f"Response finalization error: {{str(e)}}"
        logger.error(error_msg)
        return {{**state, "output": error_msg, "error": str(e)}}

def create_{agent_name.lower()}_agent():
    """Create elite {agent_name} agent with tool integration"""
    logger.info(f"Creating elite {{agent_name}} agent with tools: {tools_needed}")
    
    workflow = StateGraph({agent_name}State)
    
    # Add elite processing nodes
    workflow.add_node("select_tool", tool_selection_node)
    workflow.add_node("execute_tool", tool_execution_node)
    workflow.add_node("finalize", finalize_node)
    
    # Set entry point
    workflow.set_entry_point("select_tool")
    
    # Create elite workflow
    workflow.add_edge("select_tool", "execute_tool")
    workflow.add_edge("execute_tool", "finalize")
    workflow.add_edge("finalize", END)
    
    return workflow.compile()

# Elite usage example
if __name__ == "__main__":
    print(f"🚀 Initializing Elite {{agent_name}} Agent with Tools")
    print(f"Purpose: {purpose}")
    print(f"Available Tools: {', '.join(tools_needed)}")
    print(f"Powered by: Azure OpenAI GPT-4")
    print("=" * 80)
    
    try:
        agent = create_{agent_name.lower()}_agent()
        
        # Test with tool usage
        test_input = f"I need comprehensive help with {purpose}. Please use your tools to provide the best possible assistance."
        
        result = agent.invoke({{
            "input": test_input,
            "messages": [],
            "selected_tool": "",
            "tool_result": "",
            "output": "",
            "error": "",
            "metadata": {{}}
        }})
        
        print(f"✅ Agent Response:")
        print(f"Tool Used: {{result['selected_tool']}}")
        print(f"Response: {{result['output'][:300]}}")
        if result.get('error'):
            print(f"⚠️ Errors: {{result['error']}}")
            
    except Exception as e:
        print(f"❌ Agent execution failed: {{str(e)}}")
'''
    
    def _generate_multi_agent_system(self, agent_name: str, purpose: str, agent_types: List[str]) -> str:
        """Generate elite multi-agent supervisor system"""
        return f'''#!/usr/bin/env python3
"""
{agent_name} - Elite Multi-Agent Supervisor System
Generated by Elite Coder Agent using Core Agent Infrastructure
Purpose: {purpose}
Architecture: Supervisor + Specialized Agents
"""

from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage
from typing import TypedDict, List, Dict, Literal
from langchain_openai import AzureChatOpenAI
import logging
import json
from datetime import datetime

# Configure elite logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class {agent_name}State(TypedDict):
    """Elite state for multi-agent coordination"""
    messages: List[BaseMessage]
    task: str
    assigned_agent: str
    agent_results: Dict[str, str]
    final_output: str
    error: str
    metadata: dict
    workflow_history: List[str]

def create_azure_openai_model():
    """Create Azure OpenAI model with elite configuration"""
    return AzureChatOpenAI(
        azure_endpoint="{CoderConfig.AZURE_OPENAI_ENDPOINT}",
        api_key="{CoderConfig.OPENAI_API_KEY}",
        api_version="{CoderConfig.OPENAI_API_VERSION}",
        model="{CoderConfig.GPT4_MODEL_NAME}",
        deployment_name="{CoderConfig.GPT4_DEPLOYMENT_NAME}",
        temperature={CoderConfig.TEMPERATURE},
        max_tokens={CoderConfig.MAX_TOKENS}
    )

def elite_supervisor_node(state: {agent_name}State) -> {agent_name}State:
    """Elite supervisor with advanced task routing intelligence"""
    logger.info(f"Elite Supervisor analyzing task: {{state['task'][:100]}}")
    
    try:
        llm = create_azure_openai_model()
        
        available_agents = ["researcher", "analyzer", "creator", "reviewer"]
        
        supervisor_prompt = f\"\"\"You are the Elite Supervisor for {agent_name} system: {purpose}

MISSION: Intelligently route tasks to specialized agents for optimal results.

AVAILABLE SPECIALIZED AGENTS:
🔍 researcher: Expert in information gathering, fact-finding, and comprehensive research
📊 analyzer: Specialist in data analysis, pattern recognition, and strategic insights  
🎯 creator: Master of content creation, solution development, and implementation
✅ reviewer: Quality assurance expert for refinement, validation, and optimization

CURRENT TASK: {{state["task"]}}

ANALYSIS FRAMEWORK:
1. Task Complexity: Is this simple/moderate/complex?
2. Primary Need: Information/Analysis/Creation/Quality Control?
3. Expected Output: Research/Insights/Content/Refinement?
4. Optimal Agent: Which agent best matches the primary need?

ROUTING DECISION:
Choose the ONE agent most suited for this task. Consider the task's primary objective.
Respond with ONLY the agent name: researcher, analyzer, creator, or reviewer\"\"\"
        
        response = llm.invoke(supervisor_prompt)
        assigned = response.content.strip().lower()
        
        # Validate assignment
        if assigned in available_agents:
            workflow_entry = f"Supervisor → {{assigned}} ({{datetime.now().strftime('%H:%M:%S')}})"
            workflow_history = state.get("workflow_history", [])
            workflow_history.append(workflow_entry)
            
            logger.info(f"Task assigned to: {{assigned}}")
            return {{
                **state, 
                "assigned_agent": assigned,
                "workflow_history": workflow_history
            }}
        else:
            logger.warning(f"Invalid agent assignment: {{assigned}}, defaulting to creator")
            return {{**state, "assigned_agent": "creator"}}
            
    except Exception as e:
        logger.error(f"Supervisor error: {{str(e)}}")
        return {{**state, "assigned_agent": "creator", "error": f"Supervisor routing failed: {{str(e)}}"}}

def elite_researcher_node(state: {agent_name}State) -> {agent_name}State:
    """Elite research specialist with comprehensive information gathering"""
    logger.info(f"Elite Researcher processing: {{state['task'][:50]}}")
    
    try:
        llm = create_azure_openai_model()
        
        research_prompt = f\"\"\"You are the Elite Research Agent for {agent_name} system: {purpose}

RESEARCH MISSION: Conduct comprehensive, authoritative research on the given task.

TASK: {{state["task"]}}

RESEARCH METHODOLOGY:
1. Scope Analysis: Define research boundaries and objectives
2. Information Architecture: Structure comprehensive findings
3. Source Evaluation: Prioritize authoritative and current information
4. Synthesis: Combine multiple perspectives into coherent insights

ELITE RESEARCH STANDARDS:
- Provide comprehensive, well-structured information
- Include multiple perspectives and approaches
- Highlight key findings and critical insights
- Organize information for maximum utility
- Ensure accuracy and relevance

Deliver research results that set the foundation for exceptional outcomes.\"\"\"
        
        response = llm.invoke(research_prompt)
        
        results = state.get("agent_results", {{}})
        results["researcher"] = response.content
        
        workflow_entry = f"Researcher completed ({{datetime.now().strftime('%H:%M:%S')}})"
        workflow_history = state.get("workflow_history", [])
        workflow_history.append(workflow_entry)
        
        logger.info("Elite research completed")
        return {{
            **state, 
            "agent_results": results,
            "workflow_history": workflow_history
        }}
        
    except Exception as e:
        logger.error(f"Research error: {{str(e)}}")
        return {{**state, "error": f"Research processing failed: {{str(e)}}"}}

def elite_analyzer_node(state: {agent_name}State) -> {agent_name}State:
    """Elite analysis specialist with advanced pattern recognition"""
    logger.info(f"Elite Analyzer processing: {{state['task'][:50]}}")
    
    try:
        llm = create_azure_openai_model()
        
        previous_research = state.get("agent_results", {{}}).get("researcher", "No prior research available")
        
        analysis_prompt = f\"\"\"You are the Elite Analysis Agent for {agent_name} system: {purpose}

ANALYSIS MISSION: Provide deep analytical insights and strategic recommendations.

TASK: {{state["task"]}}
RESEARCH FOUNDATION: {{previous_research}}

ANALYTICAL FRAMEWORK:
1. Pattern Recognition: Identify key trends, relationships, and structures
2. Critical Evaluation: Assess strengths, weaknesses, opportunities, risks
3. Strategic Insights: Develop actionable recommendations and strategies
4. Synthesis: Connect diverse information into coherent frameworks

ELITE ANALYSIS STANDARDS:
- Provide data-driven insights and recommendations
- Identify non-obvious patterns and connections
- Offer strategic perspectives and actionable advice
- Support conclusions with logical reasoning
- Structure analysis for maximum decision-making value

Deliver analysis that transforms information into strategic advantage.\"\"\"
        
        response = llm.invoke(analysis_prompt)
        
        results = state.get("agent_results", {{}})
        results["analyzer"] = response.content
        
        workflow_entry = f"Analyzer completed ({{datetime.now().strftime('%H:%M:%S')}})"
        workflow_history = state.get("workflow_history", [])
        workflow_history.append(workflow_entry)
        
        logger.info("Elite analysis completed")
        return {{
            **state, 
            "agent_results": results,
            "workflow_history": workflow_history
        }}
        
    except Exception as e:
        logger.error(f"Analysis error: {{str(e)}}")
        return {{**state, "error": f"Analysis processing failed: {{str(e)}}"}}

def elite_creator_node(state: {agent_name}State) -> {agent_name}State:
    """Elite creation specialist with advanced solution development"""
    logger.info(f"Elite Creator processing: {{state['task'][:50]}}")
    
    try:
        llm = create_azure_openai_model()
        
        agent_results = state.get("agent_results", {{}})
        research_context = agent_results.get("researcher", "No research context")
        analysis_context = agent_results.get("analyzer", "No analysis context")
        
        creation_prompt = f\"\"\"You are the Elite Creator Agent for {agent_name} system: {purpose}

CREATION MISSION: Develop exceptional solutions, content, and implementations.

TASK: {{state["task"]}}
RESEARCH CONTEXT: {{research_context}}
ANALYSIS CONTEXT: {{analysis_context}}

CREATION FRAMEWORK:
1. Solution Architecture: Design comprehensive, scalable solutions
2. Implementation Strategy: Develop clear, actionable approaches
3. Quality Standards: Ensure excellence in all deliverables
4. Innovation Integration: Apply creative and advanced techniques

ELITE CREATION STANDARDS:
- Deliver production-ready, high-quality outputs
- Integrate research insights and analytical recommendations
- Provide clear implementation guidance and examples
- Ensure scalability and maintainability
- Apply best practices and innovative approaches

Create solutions that exceed expectations and deliver exceptional value.\"\"\"
        
        response = llm.invoke(creation_prompt)
        
        results = state.get("agent_results", {{}})
        results["creator"] = response.content
        
        workflow_entry = f"Creator completed ({{datetime.now().strftime('%H:%M:%S')}})"
        workflow_history = state.get("workflow_history", [])
        workflow_history.append(workflow_entry)
        
        logger.info("Elite creation completed")
        return {{
            **state, 
            "agent_results": results,
            "workflow_history": workflow_history
        }}
        
    except Exception as e:
        logger.error(f"Creation error: {{str(e)}}")
        return {{**state, "error": f"Creation processing failed: {{str(e)}}"}}

def elite_reviewer_node(state: {agent_name}State) -> {agent_name}State:
    """Elite review specialist with comprehensive quality assurance"""
    logger.info(f"Elite Reviewer processing: {{state['task'][:50]}}")
    
    try:
        llm = create_azure_openai_model()
        
        agent_results = state.get("agent_results", {{}})
        created_content = agent_results.get("creator", "No content to review")
        
        review_prompt = f\"\"\"You are the Elite Review Agent for {agent_name} system: {purpose}

REVIEW MISSION: Ensure exceptional quality and optimize all deliverables.

TASK: {{state["task"]}}
CONTENT TO REVIEW: {{created_content}}

REVIEW FRAMEWORK:
1. Quality Assessment: Evaluate completeness, accuracy, and excellence
2. Optimization Analysis: Identify improvement opportunities
3. Standards Compliance: Ensure adherence to best practices
4. Enhancement Integration: Apply refinements and optimizations

ELITE REVIEW STANDARDS:
- Conduct comprehensive quality evaluation
- Provide specific, actionable improvement recommendations
- Enhance clarity, accuracy, and effectiveness
- Ensure professional polish and presentation
- Validate against industry best practices

Deliver reviews that elevate quality to elite standards.\"\"\"
        
        response = llm.invoke(review_prompt)
        
        results = state.get("agent_results", {{}})
        results["reviewer"] = response.content
        
        workflow_entry = f"Reviewer completed ({{datetime.now().strftime('%H:%M:%S')}})"
        workflow_history = state.get("workflow_history", [])
        workflow_history.append(workflow_entry)
        
        logger.info("Elite review completed")
        return {{
            **state, 
            "agent_results": results,
            "workflow_history": workflow_history
        }}
        
    except Exception as e:
        logger.error(f"Review error: {{str(e)}}")
        return {{**state, "error": f"Review processing failed: {{str(e)}}"}}

def elite_aggregator_node(state: {agent_name}State) -> {agent_name}State:
    """Elite aggregation with intelligent synthesis"""
    logger.info(f"Elite Aggregator synthesizing results")
    
    try:
        llm = create_azure_openai_model()
        
        agent_results = state.get("agent_results", {{}})
        workflow_history = state.get("workflow_history", [])
        
        # Create comprehensive results summary
        results_summary = "\\n\\n".join([
            f"**{{agent.upper()}} AGENT CONTRIBUTION:**\\n{{result}}" 
            for agent, result in agent_results.items()
        ])
        
        workflow_summary = "\\n".join([f"- {{step}}" for step in workflow_history])
        
        aggregation_prompt = f\"\"\"You are the Elite Aggregator for {agent_name} system: {purpose}

AGGREGATION MISSION: Synthesize all agent contributions into a comprehensive, exceptional response.

ORIGINAL TASK: {{state["task"]}}

WORKFLOW EXECUTED:
{{workflow_summary}}

AGENT CONTRIBUTIONS:
{{results_summary}}

SYNTHESIS FRAMEWORK:
1. Comprehensive Integration: Weave all contributions into a unified response
2. Value Maximization: Highlight the best insights and solutions from each agent
3. Coherent Structure: Organize information for maximum clarity and impact
4. Excellence Standards: Ensure the final output exceeds all expectations

ELITE AGGREGATION STANDARDS:
- Create a seamless, professional final response
- Integrate the best elements from each specialized agent
- Provide clear structure and logical flow
- Deliver actionable, valuable insights
- Maintain the highest quality throughout

Synthesize an exceptional response that showcases the full power of multi-agent collaboration.\"\"\"
        
        response = llm.invoke(aggregation_prompt)
        
        final_metadata = {{
            "system_name": "{agent_name}",
            "purpose": "{purpose}",
            "agents_involved": list(agent_results.keys()),
            "workflow_steps": len(workflow_history),
            "timestamp": datetime.now().isoformat(),
            "model": "gpt-4",
            "architecture": "multi_agent_supervisor"
        }}
        
        logger.info("Elite aggregation completed successfully")
        return {{
            **state, 
            "final_output": response.content,
            "metadata": final_metadata
        }}
        
    except Exception as e:
        error_msg = f"Aggregation synthesis error: {{str(e)}}"
        logger.error(error_msg)
        return {{**state, "final_output": error_msg, "error": str(e)}}

def route_to_agent(state: {agent_name}State) -> str:
    """Elite routing logic for agent selection"""
    assigned = state["assigned_agent"]
    logger.info(f"Routing to elite agent: {{assigned}}")
    return assigned

def create_{agent_name.lower()}_system():
    """Create elite multi-agent supervisor system"""
    logger.info(f"Creating elite {{agent_name}} multi-agent system for: {purpose}")
    
    workflow = StateGraph({agent_name}State)
    
    # Add all elite agent nodes
    workflow.add_node("supervisor", elite_supervisor_node)
    workflow.add_node("researcher", elite_researcher_node)
    workflow.add_node("analyzer", elite_analyzer_node)
    workflow.add_node("creator", elite_creator_node)
    workflow.add_node("reviewer", elite_reviewer_node)
    workflow.add_node("aggregator", elite_aggregator_node)
    
    # Set entry point to elite supervisor
    workflow.set_entry_point("supervisor")
    
    # Add intelligent conditional routing from supervisor
    workflow.add_conditional_edges(
        "supervisor",
        route_to_agent,
        {{
            "researcher": "researcher",
            "analyzer": "analyzer", 
            "creator": "creator",
            "reviewer": "reviewer"
        }}
    )
    
    # All specialized agents route to elite aggregator
    workflow.add_edge("researcher", "aggregator")
    workflow.add_edge("analyzer", "aggregator")
    workflow.add_edge("creator", "aggregator")
    workflow.add_edge("reviewer", "aggregator")
    workflow.add_edge("aggregator", END)
    
    return workflow.compile()

# Elite usage example
if __name__ == "__main__":
    print(f"🚀 Initializing Elite {{agent_name}} Multi-Agent System")
    print(f"Purpose: {purpose}")
    print(f"Architecture: Supervisor + 4 Specialized Agents")
    print(f"Powered by: Azure OpenAI GPT-4")
    print("=" * 100)
    
    try:
        system = create_{agent_name.lower()}_system()
        
        # Test the multi-agent system
        test_task = f"Create a comprehensive solution strategy for: {purpose}. Use all available expertise to deliver exceptional results."
        
        result = system.invoke({{
            "task": test_task,
            "messages": [],
            "assigned_agent": "",
            "agent_results": {{}},
            "final_output": "",
            "error": "",
            "metadata": {{}},
            "workflow_history": []
        }})
        
        print(f"✅ Multi-Agent System Results:")
        print(f"Assigned Agent: {{result['assigned_agent']}}")
        print(f"Agents Involved: {{', '.join(result.get('agent_results', {{}}).keys())}}")
        print(f"Workflow Steps: {{len(result.get('workflow_history', []))}}")
        print(f"Final Output: {{result['final_output'][:400]}}...")
        
        if result.get('error'):
            print(f"⚠️ System Errors: {{result['error']}}")
            
    except Exception as e:
        print(f"❌ Multi-agent system execution failed: {{str(e)}}")
'''


class CodeMemoryInput(BaseModel):
    """Input for elite code memory management"""
    action: str = Field(description="Action: save, load, list, search, tag")
    module_name: str = Field(default="", description="Name of the module")
    code: str = Field(default="", description="Code content to save")
    description: str = Field(default="", description="Module description")
    tags: List[str] = Field(default=[], description="Tags for categorization")
    search_query: str = Field(default="", description="Search query for finding modules")


class CodeMemoryTool(BaseTool):
    """Elite tool for Redis-based code memory management using Core Agent infrastructure"""
    
    name: str = "code_memory"
    description: str = """Elite Redis memory management for code modules.
    Actions: save (store with tags), load (retrieve), list (show all), search (find by name/tags), tag (update tags)
    Leverages Core Agent's Redis infrastructure for persistence."""
    args_schema: type[BaseModel] = CodeMemoryInput
    
    def __init__(self, memory_manager: MemoryManager):
        super().__init__()
        self._memory_manager = memory_manager
    
    def _run(self, action: str, module_name: str = "", code: str = "", 
            description: str = "", tags: List[str] = [], search_query: str = "") -> str:
        """Execute elite memory management with Core Agent Redis"""
        try:
            if action == "save":
                if not module_name or not code:
                    return "❌ Save requires module_name and code parameters"
                
                # Use Core Agent's memory infrastructure
                memory_key = f"elite_coder:module:{module_name}"
                module_data = {
                    "code": code,
                    "description": description,
                    "tags": tags,
                    "timestamp": datetime.now().isoformat(),
                    "type": "langgraph_agent"
                }
                
                # Save to Core Agent's Redis memory
                success = self._memory_manager.save_memory(memory_key, json.dumps(module_data))
                
                if success:
                    # Add to module index
                    self._memory_manager.save_memory("elite_coder:modules", module_name, append=True)
                    
                    # Add tag indices
                    for tag in tags:
                        self._memory_manager.save_memory(f"elite_coder:tag:{tag}", module_name, append=True)
                    
                    return f"✅ Elite module '{module_name}' saved with tags: {tags}"
                else:
                    return f"❌ Failed to save module '{module_name}' to Core Agent memory"
            
            elif action == "load":
                if not module_name:
                    return "❌ Load requires module_name parameter"
                
                memory_key = f"elite_coder:module:{module_name}"
                module_json = self._memory_manager.get_memory(memory_key)
                
                if module_json:
                    module_data = json.loads(module_json)
                    return f"""✅ Elite module '{module_name}' loaded from Core Agent Redis:

📝 Description: {module_data.get('description', 'No description')}
🏷️ Tags: {', '.join(module_data.get('tags', []))}
📅 Created: {module_data.get('timestamp', 'Unknown')}
🎯 Type: {module_data.get('type', 'Unknown')}

💻 Code:
```python
{module_data['code']}
```"""
                else:
                    return f"❌ Module '{module_name}' not found in Core Agent memory"
            
            elif action == "list":
                modules_data = self._memory_manager.get_memory("elite_coder:modules")
                if modules_data:
                    if isinstance(modules_data, str):
                        modules = [modules_data] if modules_data else []
                    else:
                        modules = modules_data
                    
                    return f"📚 Elite modules in Core Agent Redis ({len(modules)}):\\n" + "\\n".join(f"  🔹 {m}" for m in modules)
                else:
                    return "📚 No elite modules found in Core Agent memory"
            
            elif action == "search":
                query = search_query or module_name
                if not query:
                    return "❌ Search requires search_query or module_name"
                
                # Search implementation using Core Agent memory
                found_modules = []
                
                # Get all modules and search in names
                modules_data = self._memory_manager.get_memory("elite_coder:modules")
                if modules_data:
                    modules = [modules_data] if isinstance(modules_data, str) else modules_data
                    found_modules.extend([m for m in modules if query.lower() in m.lower()])
                
                # Search by tags
                tag_modules = self._memory_manager.get_memory(f"elite_coder:tag:{query.lower()}")
                if tag_modules:
                    if isinstance(tag_modules, str):
                        found_modules.append(tag_modules)
                    else:
                        found_modules.extend(tag_modules)
                
                unique_modules = list(set(found_modules))
                
                if unique_modules:
                    return f"🔍 Found {len(unique_modules)} elite modules matching '{query}':\\n" + "\\n".join(f"  🎯 {m}" for m in unique_modules)
                else:
                    return f"🔍 No elite modules found matching '{query}'"
            
            else:
                return f"❌ Unknown action '{action}'. Use: save, load, list, search"
                
        except Exception as e:
            return f"❌ Core Agent memory operation error: {str(e)}"


class EliteCoderAgent:
    """Elite Coder Agent leveraging Core Agent Infrastructure"""
    
    def __init__(self, session_id: str = "default"):
        self.session_id = session_id
        
        print(f"🚀 Initializing Elite Coder Agent with Core Agent Infrastructure")
        
        # Create Core Agent configuration for Coder Agent
        self.config = AgentConfig(
            name="EliteCoderAgent",
            model=self._create_azure_openai_model(),
            enable_memory=True,
            memory_backend="redis",
            redis_url="redis://localhost:6379/3",  # DB 3 for elite coder
            memory_types=["short_term", "long_term"],
            tools=self._create_elite_tools(),
            system_prompt=self._get_elite_system_prompt(),
            max_tokens=CoderConfig.MAX_TOKENS
        )
        
        # Initialize Core Agent managers
        try:
            self.memory_manager = MemoryManager(self.config)
            print(f"✅ Core Agent infrastructure initialized successfully")
        except Exception as e:
            print(f"⚠️ Core Agent managers initialization warning: {e}")
            self.memory_manager = None
        
        # Direct Azure OpenAI for immediate use
        self.llm = self._create_azure_openai_model()
        
        print(f"🤖 Elite Coder Agent ready with session: {session_id}")
        print(f"🔧 Available tools: {[tool.name for tool in self.config.tools]}")
        print(f"💾 Memory backend: {self.config.memory_backend}")
        print(f"🧠 Model: Azure OpenAI {CoderConfig.GPT4_MODEL_NAME}")
    
    def _create_azure_openai_model(self):
        """Create Azure OpenAI model with exact user specifications"""
        return AzureChatOpenAI(
            azure_endpoint=CoderConfig.AZURE_OPENAI_ENDPOINT,
            api_key=CoderConfig.OPENAI_API_KEY,
            api_version=CoderConfig.OPENAI_API_VERSION,
            model=CoderConfig.GPT4_MODEL_NAME,
            deployment_name=CoderConfig.GPT4_DEPLOYMENT_NAME,
            temperature=CoderConfig.TEMPERATURE,
            max_tokens=CoderConfig.MAX_TOKENS
        )
    
    def _create_elite_tools(self):
        """Create elite tools combining Core Agent tools with specialized ones"""
        tools = []
        
        # Get Core Agent's Python coding tools
        try:
            core_tools = create_python_coding_tools()
            tools.extend(core_tools)
            print(f"✅ Added {len(core_tools)} Core Agent tools")
        except Exception as e:
            print(f"⚠️ Core Agent tools not available: {e}")
        
        # Add specialized Elite Coder tools
        elite_tools = [
            LangGraphTemplateTool()
        ]
        
        # Add memory tool if memory manager is available
        if hasattr(self, 'memory_manager') and self.memory_manager:
            elite_tools.append(CodeMemoryTool(self.memory_manager))
        
        tools.extend(elite_tools)
        
        return tools
    
    def _get_elite_system_prompt(self) -> str:
        """Elite system prompt for world-class coding"""
        return """You are an ELITE CODER AGENT, the absolute pinnacle of AI coding expertise, built on Core Agent's powerful infrastructure.

🎯 YOUR SUPREME MISSION:
Create exceptional, production-ready LangGraph agents, tools, and multi-agent systems that showcase the absolute pinnacle of development excellence.

🏗️ CORE AGENT INFRASTRUCTURE MASTERY:
- **Memory Excellence**: Persistent Redis memory via Core Agent's MemoryManager
- **Tool Integration**: Seamless use of Core Agent's tool ecosystem  
- **Configuration Mastery**: Advanced AgentConfig patterns and best practices
- **Azure OpenAI GPT-4**: Direct access with perfect deployment configuration

🧠 ELITE SPECIALIZATIONS:
1. **Simple LangGraph Agents**: Lightning-fast, robust single-purpose agents
2. **Tool-Enhanced Agents**: Sophisticated agents with custom tool integration  
3. **Multi-Agent Systems**: Complex supervisor-worker architectures with perfect coordination
4. **Complete Ecosystems**: Full project structures with enterprise-grade organization

💡 CORE AGENT PRINCIPLES:
- **No Mocks Ever**: Real implementations using actual Azure OpenAI and Redis
- **Memory Continuity**: Remember and build upon previous code modules using Redis
- **Infrastructure Leverage**: Maximum utilization of Core Agent's managers and tools
- **Production Excellence**: Enterprise-grade code with comprehensive error handling
- **Modular Architecture**: Reusable, composable, maintainable components

🛠️ YOUR ELITE TOOLSET:
- **langgraph_generator**: Generate complete agent templates (simple, with_tools, multi_agent)
- **code_memory**: Save/load/search code modules in Core Agent's Redis with intelligent tagging
- **python_executor**: Execute and test code safely with detailed feedback (from Core Agent)
- **file_manager**: Project and file management with proper structure (from Core Agent)

📝 ELITE WORKFLOW MASTERY:
1. **Deep Analysis**: Understand requirements with surgical precision
2. **Architectural Excellence**: Design using advanced LangGraph + Core Agent patterns
3. **Code Generation**: Create pristine, documented, production-ready code
4. **Memory Integration**: Save modules with intelligent tagging for future use
5. **Validation**: Execute and test all generated code
6. **Example Provision**: Provide complete working examples

🔥 SIGNATURE EXCELLENCE:
- Every response includes working, tested code
- All modules are saved to Core Agent's Redis memory with intelligent tags
- Complete project structures when requested
- Real Azure OpenAI configuration (never mocked)
- Comprehensive error handling and logging
- Core Agent infrastructure integration in every component

🚀 EXAMPLE PROMPT MASTERY:
1. "Create a simple LangGraph agent for [task]" → Generate complete agent with Core Agent integration
2. "Build an agent with [tools] for [purpose]" → Create sophisticated tool-enhanced agent using Core Agent tools
3. "Design a multi-agent system for [complex_task]" → Build full supervisor architecture with Core Agent memory

💎 ELITE TRAITS:
- Leverage Core Agent's MemoryManager for persistent Redis storage
- Integrate Core Agent's tools seamlessly with custom specialized tools
- Use AgentConfig patterns for consistent, maintainable architecture
- Apply Core Agent's best practices throughout all generated code
- Ensure perfect compatibility with Core Agent ecosystem

You are the ULTIMATE coder agent, built on Core Agent's rock-solid foundation. Every line of code you create showcases the perfect marriage of LangGraph excellence and Core Agent infrastructure mastery!"""
    
    def get_example_prompts(self) -> Dict[str, str]:
        """Get example prompts showcasing elite capabilities"""
        return {
            "simple_agent": """Create a simple LangGraph agent for Python code analysis and optimization. 
The agent should use Core Agent's infrastructure, Azure OpenAI GPT-4, and save results to Redis memory.""",
            
            "agent_with_tools": """Build a LangGraph agent with tools for automated testing. 
Include tools for: test generation, code execution, and result analysis. 
Use Core Agent's tool ecosystem and memory system for persistence.""",
            
            "multi_agent_system": """Design a multi-agent system for full-stack development with:
- A backend specialist agent for API development
- A frontend specialist agent for UI creation  
- A testing specialist agent for QA automation
- A DevOps specialist agent for deployment
- A supervisor that coordinates the entire development workflow
Include Core Agent memory integration and proper task routing."""
        }
    
    def chat(self, message: str) -> str:
        """Elite chat interface with Core Agent memory integration"""
        try:
            # Get conversation context from Core Agent memory
            memory_context = ""
            if self.memory_manager:
                try:
                    memory_key = f"elite_coder:conversation:{self.session_id}"
                    context_data = self.memory_manager.get_memory(memory_key)
                    if context_data:
                        memory_context = f"Previous conversation context: {context_data[:500]}..."
                except Exception as e:
                    print(f"⚠️ Memory context retrieval warning: {e}")
            
            # Build elite conversation
            messages = [SystemMessage(content=self._get_elite_system_prompt())]
            
            # Add memory context if available
            if memory_context:
                messages.append(HumanMessage(content=memory_context))
            
            # Add current message
            messages.append(HumanMessage(content=message))
            
            # Get elite AI response
            response = self.llm.invoke(messages)
            
            # Save interaction to Core Agent memory
            if self.memory_manager:
                try:
                    conversation_entry = {
                        "user": message,
                        "agent": response.content,
                        "timestamp": datetime.now().isoformat()
                    }
                    memory_key = f"elite_coder:conversation:{self.session_id}"
                    self.memory_manager.save_memory(memory_key, json.dumps(conversation_entry))
                except Exception as e:
                    print(f"⚠️ Memory save warning: {e}")
            
            return response.content
            
        except Exception as e:
            error_msg = f"❌ Elite Coder Agent error: {str(e)}"
            print(error_msg)
            return error_msg
    
    def generate_complete_agent(self, template_type: str, agent_name: str, purpose: str, 
                              tools_needed: List[str] = []) -> Dict[str, Any]:
        """Generate complete agent with Core Agent integration"""
        try:
            print(f"🎯 Generating elite {template_type} agent: {agent_name}")
            
            # Generate the agent code
            template_tool = LangGraphTemplateTool()
            agent_code = template_tool._run(template_type, agent_name, purpose, tools_needed)
            
            # Save to Core Agent memory with intelligent tags
            tags = [template_type, "langgraph", "elite_agent", "core_agent_integrated"]
            if tools_needed:
                tags.extend(["with_tools"] + tools_needed)
            
            save_result = "Memory not available"
            if self.memory_manager:
                try:
                    memory_tool = CodeMemoryTool(self.memory_manager)
                    save_result = memory_tool._run("save", f"{agent_name.lower()}_agent", 
                                                 agent_code, purpose, tags)
                except Exception as e:
                    save_result = f"Memory save warning: {str(e)}"
            
            # Test the code using Core Agent tools
            test_result = "Testing not available"
            try:
                # Find Python executor tool from Core Agent tools
                python_tool = None
                for tool in self.config.tools:
                    if hasattr(tool, 'name') and 'python' in tool.name.lower():
                        python_tool = tool
                        break
                
                if python_tool:
                    test_result = python_tool._run(agent_code, timeout=10)
                else:
                    test_result = "✅ Code generated (Python executor not available for testing)"
            except Exception as e:
                test_result = f"Testing warning: {str(e)}"
            
            return {
                "agent_code": agent_code,
                "save_result": save_result,
                "test_result": test_result,
                "template_type": template_type,
                "agent_name": agent_name,
                "purpose": purpose,
                "tools": tools_needed,
                "tags": tags,
                "core_agent_integrated": True
            }
            
        except Exception as e:
            return {
                "error": f"Elite agent generation failed: {str(e)}",
                "agent_code": "",
                "save_result": "",
                "test_result": "",
                "core_agent_integrated": False
            }


def create_elite_coder_agent(session_id: str = None) -> EliteCoderAgent:
    """Factory function to create Elite Coder Agent with Core Agent infrastructure"""
    if session_id is None:
        session_id = f"elite_coder_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    return EliteCoderAgent(session_id)


def demo_elite_coder():
    """Demonstrate Elite Coder Agent with Core Agent integration"""
    print("🚀 ELITE CODER AGENT - CORE AGENT INTEGRATION DEMO")
    print("=" * 100)
    
    try:
        # Create elite agent
        agent = create_elite_coder_agent("demo_session")
        
        # Show example prompts
        examples = agent.get_example_prompts()
        print("\\n📝 ELITE EXAMPLE PROMPTS:")
        for level, prompt in examples.items():
            print(f"\\n🎯 {level.upper().replace('_', ' ')}:")
            print(f"   {prompt}")
        
        # Test basic functionality
        print("\\n🧪 TESTING BASIC FUNCTIONALITY:")
        test_response = agent.chat("Create a simple LangGraph agent for data processing")
        print(f"✅ Chat Response: {test_response[:200]}...")
        
        # Test agent generation
        print("\\n🎯 TESTING AGENT GENERATION:")
        result = agent.generate_complete_agent(
            template_type="simple",
            agent_name="DataProcessor", 
            purpose="Process and analyze data efficiently",
            tools_needed=[]
        )
        
        print(f"✅ Agent Generated: {result['agent_name']}")
        print(f"💾 Memory Save: {result['save_result']}")
        print(f"🧪 Test Result: {result['test_result'][:100]}...")
        
        print("\\n" + "=" * 100)
        print("✅ Elite Coder Agent with Core Agent integration ready!")
        print("💎 Powered by: Azure OpenAI GPT-4 + Core Agent Infrastructure + Redis Memory")
        
    except Exception as e:
        print(f"❌ Demo failed: {str(e)}")


if __name__ == "__main__":
    demo_elite_coder()